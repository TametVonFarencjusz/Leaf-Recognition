{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad6dc0b-c30f-4300-9e62-dad86e69148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "982f34b8-1c35-4767-8c19-5ee446aab58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAll pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. \\nThe images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define data transformations for data augmentation and normalization\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. \n",
    "The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c86d400-80fe-41bf-bd98-c4d92f6036b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory\n",
    "data_dir = 'dataset'\n",
    "\n",
    "# Create data loaders\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "#image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "678a888b-f751-4e87-be61-55a92be361c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 4411, 'val': 490}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alstonia_scholaris_diseased',\n",
       " 'alstonia_scholaris_healthy',\n",
       " 'arjun_diseased',\n",
       " 'arjun_healthy',\n",
       " 'bael_diseased',\n",
       " 'basil_healthy',\n",
       " 'chinar_diseased',\n",
       " 'chinar_healthy',\n",
       " 'gauva_diseased',\n",
       " 'gauva_healthy',\n",
       " 'jamun_diseased',\n",
       " 'jamun_healthy',\n",
       " 'jatropha_diseased',\n",
       " 'jatropha_healthy',\n",
       " 'lemon_diseased',\n",
       " 'lemon_healthy',\n",
       " 'mango_diseased',\n",
       " 'mango_healthy',\n",
       " 'pomegranate_diseased',\n",
       " 'pomegranate_healthy',\n",
       " 'pongamia_pinnata_diseased',\n",
       " 'pongamia_pinnata_healthy']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "print(dataset_sizes)\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7f4d89f-c66d-473a-b526-d4cbb991339b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained ResNet-18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final classification layer\n",
    "for name, param in model.named_parameters():\n",
    "    if \"fc\" in name:  # Unfreeze the final classification layer\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # Use all parameters\n",
    "\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe788bcb-77d7-4c86-9f4b-80868208b8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.2260 Acc: 0.3954\n",
      "val Loss: 0.7381 Acc: 0.7633\n",
      "train Loss: 1.2603 Acc: 0.6069\n",
      "val Loss: 0.5384 Acc: 0.8245\n",
      "train Loss: 1.0543 Acc: 0.6611\n",
      "val Loss: 0.4053 Acc: 0.8735\n",
      "train Loss: 0.9846 Acc: 0.6862\n",
      "val Loss: 0.4014 Acc: 0.8694\n",
      "train Loss: 0.9193 Acc: 0.7057\n",
      "val Loss: 0.4553 Acc: 0.8592\n",
      "train Loss: 0.8949 Acc: 0.7155\n",
      "val Loss: 0.3632 Acc: 0.8755\n",
      "train Loss: 0.8599 Acc: 0.7316\n",
      "val Loss: 0.3380 Acc: 0.8939\n",
      "train Loss: 0.8603 Acc: 0.7309\n",
      "val Loss: 0.2747 Acc: 0.9020\n",
      "train Loss: 0.8484 Acc: 0.7311\n",
      "val Loss: 0.3109 Acc: 0.8959\n",
      "train Loss: 0.8168 Acc: 0.7325\n",
      "val Loss: 0.3406 Acc: 0.8918\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c696019-b8ca-4cac-9f5f-d350e7285265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), '30E.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057bb6d-9592-4809-af3a-fb05ef595477",
   "metadata": {},
   "source": [
    "# Classification on Unseen Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98f514-6ad8-46cb-b9b6-f56f7da59476",
   "metadata": {},
   "source": [
    "To use the saved model to classify unseen images, you need to load the model and then apply it to the new images for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a509530-539e-4e35-8a1b-181215da720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "# Load the saved model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 1000)  # Adjust to match the original model's output units\n",
    "model.load_state_dict(torch.load('50E.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a new model with the correct final layer\n",
    "new_model = models.resnet18(pretrained=True)\n",
    "new_model.fc = nn.Linear(new_model.fc.in_features, 2)  # Adjust to match the desired output units\n",
    "\n",
    "# Copy the weights and biases from the loaded model to the new model\n",
    "new_model.fc.weight.data = model.fc.weight.data[0:2]  # Copy only the first 2 output units\n",
    "new_model.fc.bias.data = model.fc.bias.data[0:2]\n",
    "\n",
    "# Map the predicted class to the class name\n",
    "class_names = ['alstonia_scholaris_diseased',\n",
    " 'alstonia_scholaris_healthy',\n",
    " 'arjun_diseased',\n",
    " 'arjun_healthy',\n",
    " 'bael_diseased',\n",
    " 'basil_healthy',\n",
    " 'chinar_diseased',\n",
    " 'chinar_healthy',\n",
    " 'gauva_diseased',\n",
    " 'gauva_healthy',\n",
    " 'jamun_diseased',\n",
    " 'jamun_healthy',\n",
    " 'jatropha_diseased',\n",
    " 'jatropha_healthy',\n",
    " 'lemon_diseased',\n",
    " 'lemon_healthy',\n",
    " 'mango_diseased',\n",
    " 'mango_healthy',\n",
    " 'pomegranate_diseased',\n",
    " 'pomegranate_healthy',\n",
    " 'pongamia_pinnata_diseased',\n",
    " 'pongamia_pinnata_healthy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557a6cc-c536-44b7-bfad-52f3d33e1480",
   "metadata": {},
   "source": [
    "Prepare your new image for classification. You should use the same data transformations you used during training. Here's an example of how to prepare an image for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1445dcf2-2311-4025-80e9-2b57ccf559e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files=['0014_0230.JPG', '0014_0231.JPG', '0014_0232.JPG', '0014_0233.JPG', '0014_0234.JPG', '0014_0235.JPG', '0014_0236.JPG', '0014_0237.JPG', '0014_0238.JPG', '0014_0239.JPG', '0014_0240.JPG', '0014_0241.JPG', '0014_0242.JPG', '0014_0243.JPG', '0014_0244.JPG', '0014_0245.JPG', '0014_0246.JPG', '0014_0247.JPG', '0014_0248.JPG', '0014_0249.JPG', '0014_0250.JPG', '0014_0251.JPG', '0014_0252.JPG', '0014_0253.JPG', '0014_0254.JPG']\n",
      "len(files)=25\n",
      "The predicted class is: arjun_healthy should be alstonia_scholaris_diseased\n",
      "The predicted class is: arjun_diseased should be alstonia_scholaris_diseased\n",
      "The predicted class is: alstonia_scholaris_healthy should be alstonia_scholaris_diseased\n",
      "The predicted class is: alstonia_scholaris_healthy should be alstonia_scholaris_diseased\n",
      "The predicted class is: alstonia_scholaris_healthy should be alstonia_scholaris_diseased\n",
      "files=['0003_0162.JPG', '0003_0163.JPG', '0003_0164.JPG', '0003_0165.JPG', '0003_0166.JPG', '0003_0167.JPG', '0003_0168.JPG', '0003_0169.JPG', '0003_0170.JPG', '0003_0171.JPG', '0003_0172.JPG', '0003_0173.JPG', '0003_0174.JPG', '0003_0175.JPG', '0003_0176.JPG', '0003_0177.JPG', '0003_0178.JPG', '0003_0179.JPG']\n",
      "len(files)=18\n",
      "The predicted class is: gauva_healthy should be alstonia_scholaris_healthy\n",
      "The predicted class is: pomegranate_healthy should be alstonia_scholaris_healthy\n",
      "files=['0013_0210.JPG', '0013_0211.JPG', '0013_0212.JPG', '0013_0213.JPG', '0013_0214.JPG', '0013_0215.JPG', '0013_0216.JPG', '0013_0217.JPG', '0013_0218.JPG', '0013_0219.JPG', '0013_0220.JPG', '0013_0221.JPG', '0013_0222.JPG', '0013_0223.JPG', '0013_0224.JPG', '0013_0225.JPG', '0013_0226.JPG', '0013_0227.JPG', '0013_0228.JPG', '0013_0229.JPG', '0013_0230.JPG', '0013_0231.JPG', '0013_0232.JPG']\n",
      "len(files)=23\n",
      "The predicted class is: gauva_healthy should be arjun_diseased\n",
      "The predicted class is: pongamia_pinnata_diseased should be arjun_diseased\n",
      "files=['0002_0199.JPG', '0002_0200.JPG', '0002_0201.JPG', '0002_0202.JPG', '0002_0203.JPG', '0002_0204.JPG', '0002_0205.JPG', '0002_0206.JPG', '0002_0207.JPG', '0002_0208.JPG', '0002_0209.JPG', '0002_0210.JPG', '0002_0211.JPG', '0002_0212.JPG', '0002_0213.JPG', '0002_0214.JPG', '0002_0215.JPG', '0002_0216.JPG', '0002_0217.JPG', '0002_0218.JPG', '0002_0219.JPG', '0002_0220.JPG']\n",
      "len(files)=22\n",
      "The predicted class is: arjun_diseased should be arjun_healthy\n",
      "The predicted class is: pomegranate_healthy should be arjun_healthy\n",
      "The predicted class is: arjun_diseased should be arjun_healthy\n",
      "The predicted class is: pomegranate_healthy should be arjun_healthy\n",
      "The predicted class is: pomegranate_healthy should be arjun_healthy\n",
      "The predicted class is: pomegranate_healthy should be arjun_healthy\n",
      "The predicted class is: pomegranate_healthy should be arjun_healthy\n",
      "The predicted class is: pomegranate_healthy should be arjun_healthy\n",
      "files=['0016_0107.JPG', '0016_0108.JPG', '0016_0109.JPG', '0016_0110.JPG', '0016_0111.JPG', '0016_0112.JPG', '0016_0113.JPG', '0016_0114.JPG', '0016_0115.JPG', '0016_0116.JPG', '0016_0117.JPG', '0016_0118.JPG']\n",
      "len(files)=12\n",
      "files=['0008_0134.JPG', '0008_0135.JPG', '0008_0136.JPG', '0008_0137.JPG', '0008_0138.JPG', '0008_0139.JPG', '0008_0140.JPG', '0008_0141.JPG', '0008_0142.JPG', '0008_0143.JPG', '0008_0144.JPG', '0008_0145.JPG', '0008_0146.JPG', '0008_0147.JPG', '0008_0148.JPG']\n",
      "len(files)=15\n",
      "files=['0022_0109.JPG', '0022_0110.JPG', '0022_0111.JPG', '0022_0112.JPG', '0022_0113.JPG', '0022_0114.JPG', '0022_0115.JPG', '0022_0116.JPG', '0022_0117.JPG', '0022_0118.JPG', '0022_0119.JPG', '0022_0120.JPG']\n",
      "len(files)=12\n",
      "The predicted class is: chinar_healthy should be chinar_diseased\n",
      "files=['0011_0093.JPG', '0011_0094.JPG', '0011_0095.JPG', '0011_0096.JPG', '0011_0097.JPG', '0011_0098.JPG', '0011_0099.JPG', '0011_0100.JPG', '0011_0101.JPG', '0011_0102.JPG', '0011_0103.JPG']\n",
      "len(files)=11\n",
      "files=['0015_0129.JPG', '0015_0130.JPG', '0015_0131.JPG', '0015_0132.JPG', '0015_0133.JPG', '0015_0134.JPG', '0015_0135.JPG', '0015_0136.JPG', '0015_0137.JPG', '0015_0138.JPG', '0015_0139.JPG', '0015_0140.JPG', '0015_0141.JPG', '0015_0142.JPG']\n",
      "len(files)=14\n",
      "The predicted class is: gauva_healthy should be gauva_diseased\n",
      "The predicted class is: gauva_healthy should be gauva_diseased\n",
      "The predicted class is: gauva_healthy should be gauva_diseased\n",
      "The predicted class is: gauva_healthy should be gauva_diseased\n",
      "The predicted class is: gauva_healthy should be gauva_diseased\n",
      "files=['0004_0230.JPG', '0004_0231.JPG', '0004_0232.JPG', '0004_0233.JPG', '0004_0234.JPG', '0004_0235.JPG', '0004_0236.JPG', '0004_0237.JPG', '0004_0238.JPG', '0004_0239.JPG', '0004_0240.JPG', '0004_0241.JPG', '0004_0242.JPG', '0004_0243.JPG', '0004_0244.JPG', '0004_0245.JPG', '0004_0246.JPG', '0004_0247.JPG', '0004_0248.JPG', '0004_0249.JPG', '0004_0250.JPG', '0004_0251.JPG', '0004_0252.JPG', '0004_0253.JPG', '0004_0254.JPG', '0004_0255.JPG', '0004_0256.JPG', '0004_0257.JPG', '0004_0258.JPG', '0004_0259.JPG', '0004_0260.JPG', '0004_0261.JPG', '0004_0262.JPG', '0004_0263.JPG', '0004_0264.JPG', '0004_0265.JPG', '0004_0266.JPG', '0004_0267.JPG', '0004_0268.JPG', '0004_0269.JPG', '0004_0270.JPG', '0004_0271.JPG', '0004_0272.JPG', '0004_0273.JPG', '0004_0274.JPG', '0004_0275.JPG', '0004_0276.JPG', '0004_0277.JPG']\n",
      "len(files)=48\n",
      "files=['0017_0312.JPG', '0017_0313.JPG', '0017_0314.JPG', '0017_0315.JPG', '0017_0316.JPG', '0017_0317.JPG', '0017_0318.JPG', '0017_0319.JPG', '0017_0320.JPG', '0017_0321.JPG', '0017_0322.JPG', '0017_0323.JPG', '0017_0324.JPG', '0017_0325.JPG', '0017_0326.JPG', '0017_0327.JPG', '0017_0328.JPG', '0017_0329.JPG', '0017_0330.JPG', '0017_0331.JPG', '0017_0332.JPG', '0017_0333.JPG', '0017_0334.JPG', '0017_0335.JPG', '0017_0336.JPG', '0017_0337.JPG', '0017_0338.JPG', '0017_0339.JPG', '0017_0340.JPG', '0017_0341.JPG', '0017_0342.JPG', '0017_0343.JPG', '0017_0344.JPG', '0017_0345.JPG']\n",
      "len(files)=34\n",
      "The predicted class is: jatropha_healthy should be jamun_diseased\n",
      "The predicted class is: jamun_healthy should be jamun_diseased\n",
      "The predicted class is: pongamia_pinnata_diseased should be jamun_diseased\n",
      "files=['0005_0252.JPG', '0005_0253.JPG', '0005_0254.JPG', '0005_0255.JPG', '0005_0256.JPG', '0005_0257.JPG', '0005_0258.JPG', '0005_0259.JPG', '0005_0260.JPG', '0005_0261.JPG', '0005_0262.JPG', '0005_0263.JPG', '0005_0264.JPG', '0005_0265.JPG', '0005_0266.JPG', '0005_0267.JPG', '0005_0268.JPG', '0005_0269.JPG', '0005_0270.JPG', '0005_0271.JPG', '0005_0272.JPG', '0005_0273.JPG', '0005_0274.JPG', '0005_0275.JPG', '0005_0276.JPG', '0005_0277.JPG', '0005_0278.JPG', '0005_0279.JPG']\n",
      "len(files)=28\n",
      "The predicted class is: jamun_diseased should be jamun_healthy\n",
      "The predicted class is: jamun_diseased should be jamun_healthy\n",
      "The predicted class is: jamun_diseased should be jamun_healthy\n",
      "The predicted class is: gauva_healthy should be jamun_healthy\n",
      "The predicted class is: jamun_diseased should be jamun_healthy\n",
      "The predicted class is: jamun_diseased should be jamun_healthy\n",
      "The predicted class is: jamun_diseased should be jamun_healthy\n",
      "The predicted class is: jamun_diseased should be jamun_healthy\n",
      "The predicted class is: jamun_diseased should be jamun_healthy\n",
      "The predicted class is: jamun_diseased should be jamun_healthy\n",
      "files=['0018_0113.JPG', '0018_0114.JPG', '0018_0115.JPG', '0018_0116.JPG', '0018_0117.JPG', '0018_0118.JPG', '0018_0119.JPG', '0018_0120.JPG', '0018_0121.JPG', '0018_0122.JPG', '0018_0123.JPG', '0018_0124.JPG']\n",
      "len(files)=12\n",
      "The predicted class is: jatropha_healthy should be jatropha_diseased\n",
      "The predicted class is: jatropha_healthy should be jatropha_diseased\n",
      "files=['0006_0121.JPG', '0006_0122.JPG', '0006_0123.JPG', '0006_0124.JPG', '0006_0125.JPG', '0006_0126.JPG', '0006_0127.JPG', '0006_0128.JPG', '0006_0129.JPG', '0006_0130.JPG', '0006_0131.JPG', '0006_0132.JPG', '0006_0133.JPG']\n",
      "len(files)=13\n",
      "The predicted class is: jatropha_diseased should be jatropha_healthy\n",
      "The predicted class is: jatropha_diseased should be jatropha_healthy\n",
      "files=['0021_0070.JPG', '0021_0071.JPG', '0021_0072.JPG', '0021_0073.JPG', '0021_0074.JPG', '0021_0075.JPG', '0021_0076.JPG', '0021_0077.JPG']\n",
      "len(files)=8\n",
      "The predicted class is: lemon_healthy should be lemon_diseased\n",
      "files=['0010_0144.JPG', '0010_0145.JPG', '0010_0146.JPG', '0010_0147.JPG', '0010_0148.JPG', '0010_0149.JPG', '0010_0150.JPG', '0010_0151.JPG', '0010_0152.JPG', '0010_0153.JPG', '0010_0154.JPG', '0010_0155.JPG', '0010_0156.JPG', '0010_0157.JPG', '0010_0158.JPG', '0010_0159.JPG']\n",
      "len(files)=16\n",
      "The predicted class is: arjun_healthy should be lemon_healthy\n",
      "files=['0012_0240.JPG', '0012_0241.JPG', '0012_0242.JPG', '0012_0243.JPG', '0012_0244.JPG', '0012_0245.JPG', '0012_0246.JPG', '0012_0247.JPG', '0012_0248.JPG', '0012_0249.JPG', '0012_0250.JPG', '0012_0251.JPG', '0012_0252.JPG', '0012_0253.JPG', '0012_0254.JPG', '0012_0255.JPG', '0012_0256.JPG', '0012_0257.JPG', '0012_0258.JPG', '0012_0259.JPG', '0012_0260.JPG', '0012_0261.JPG', '0012_0262.JPG', '0012_0263.JPG', '0012_0264.JPG', '0012_0265.JPG']\n",
      "len(files)=26\n",
      "files=['0001_0154.JPG', '0001_0155.JPG', '0001_0156.JPG', '0001_0157.JPG', '0001_0158.JPG', '0001_0159.JPG', '0001_0160.JPG', '0001_0161.JPG', '0001_0162.JPG', '0001_0163.JPG', '0001_0164.JPG', '0001_0165.JPG', '0001_0166.JPG', '0001_0167.JPG', '0001_0168.JPG', '0001_0169.JPG', '0001_0170.JPG']\n",
      "len(files)=17\n",
      "The predicted class is: mango_diseased should be mango_healthy\n",
      "files=['0020_0246.JPG', '0020_0247.JPG', '0020_0248.JPG', '0020_0249.JPG', '0020_0250.JPG', '0020_0251.JPG', '0020_0252.JPG', '0020_0253.JPG', '0020_0254.JPG', '0020_0255.JPG', '0020_0256.JPG', '0020_0257.JPG', '0020_0258.JPG', '0020_0259.JPG', '0020_0260.JPG', '0020_0261.JPG', '0020_0262.JPG', '0020_0263.JPG', '0020_0264.JPG', '0020_0265.JPG', '0020_0266.JPG', '0020_0267.JPG', '0020_0268.JPG', '0020_0269.JPG', '0020_0270.JPG', '0020_0271.JPG', '0020_0272.JPG']\n",
      "len(files)=27\n",
      "The predicted class is: pomegranate_healthy should be pomegranate_diseased\n",
      "The predicted class is: pomegranate_healthy should be pomegranate_diseased\n",
      "The predicted class is: pomegranate_healthy should be pomegranate_diseased\n",
      "files=['0009_0259.JPG', '0009_0260.JPG', '0009_0261.JPG', '0009_0262.JPG', '0009_0263.JPG', '0009_0264.JPG', '0009_0265.JPG', '0009_0266.JPG', '0009_0267.JPG', '0009_0268.JPG', '0009_0269.JPG', '0009_0270.JPG', '0009_0271.JPG', '0009_0272.JPG', '0009_0273.JPG', '0009_0274.JPG', '0009_0275.JPG', '0009_0276.JPG', '0009_0277.JPG', '0009_0278.JPG', '0009_0279.JPG', '0009_0280.JPG', '0009_0281.JPG', '0009_0282.JPG', '0009_0283.JPG', '0009_0284.JPG', '0009_0285.JPG', '0009_0286.JPG', '0009_0287.JPG']\n",
      "len(files)=29\n",
      "files=['0019_0254.JPG', '0019_0255.JPG', '0019_0256.JPG', '0019_0257.JPG', '0019_0258.JPG', '0019_0259.JPG', '0019_0260.JPG', '0019_0261.JPG', '0019_0262.JPG', '0019_0263.JPG', '0019_0264.JPG', '0019_0265.JPG', '0019_0266.JPG', '0019_0267.JPG', '0019_0268.JPG', '0019_0269.JPG', '0019_0270.JPG', '0019_0271.JPG', '0019_0272.JPG', '0019_0273.JPG', '0019_0274.JPG', '0019_0275.JPG', '0019_0276.JPG']\n",
      "len(files)=23\n",
      "The predicted class is: pomegranate_diseased should be pongamia_pinnata_diseased\n",
      "files=['0007_0296.JPG', '0007_0297.JPG', '0007_0298.JPG', '0007_0299.JPG', '0007_0300.JPG', '0007_0301.JPG', '0007_0302.JPG', '0007_0303.JPG', '0007_0304.JPG', '0007_0305.JPG', '0007_0306.JPG', '0007_0307.JPG', '0007_0308.JPG', '0007_0309.JPG', '0007_0310.JPG', '0007_0311.JPG', '0007_0312.JPG', '0007_0313.JPG', '0007_0314.JPG', '0007_0315.JPG', '0007_0316.JPG', '0007_0317.JPG', '0007_0318.JPG', '0007_0319.JPG', '0007_0320.JPG', '0007_0321.JPG', '0007_0322.JPG', '0007_0323.JPG', '0007_0324.JPG', '0007_0325.JPG', '0007_0326.JPG', '0007_0327.JPG', '0007_0328.JPG', '0007_0329.JPG', '0007_0330.JPG', '0007_0331.JPG', '0007_0332.JPG']\n",
      "len(files)=37\n",
      "counter = 470\n",
      "correct = 423\n",
      "wrong = 47\n",
      "correct_leaf = 453\n",
      "wrong_leaf = 17\n"
     ]
    }
   ],
   "source": [
    "folders = os.listdir(\"dataset/test\")\n",
    "\n",
    "SHOWIMAGE = False\n",
    "\n",
    "counter = 0\n",
    "correct = 0\n",
    "correct_leaf = 0\n",
    "wrong = 0\n",
    "wrong_leaf = 0\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(\"dataset/test/\"+folder)\n",
    "    print(f'{folder}')\n",
    "    print(f'{files=}')\n",
    "    print(f'{len(files)=}')\n",
    "    for file in files:\n",
    "        # Load and preprocess the unseen image\n",
    "        image_path = \"dataset/test/\"+folder+\"/\"+file  # Replace with the path to your image\n",
    "        image = Image.open(image_path)\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        input_tensor = preprocess(image)\n",
    "        input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "\n",
    "        #Perform inference using the model:\n",
    "        with torch.no_grad():\n",
    "            output = model(input_batch)\n",
    "\n",
    "        # Get the predicted class\n",
    "        _, predicted_class = output.max(1)\n",
    "\n",
    "        predicted_class_name = class_names[predicted_class.item()]\n",
    "\n",
    "        counter += 1\n",
    "        if predicted_class_name == folder:\n",
    "            correct += 1\n",
    "            #print(f'The predicted class is: {predicted_class_name}')\n",
    "        else:\n",
    "            wrong += 1\n",
    "            print(f'The predicted class is: {predicted_class_name} should be {folder}')\n",
    "            \n",
    "        if predicted_class_name.replace('_healthy', '').replace('_diseased', '') == folder.replace('_healthy', '').replace('_diseased', ''):\n",
    "            correct_leaf += 1\n",
    "            #print(f'The predicted class is: {predicted_class_name}')\n",
    "        else:\n",
    "            wrong_leaf += 1\n",
    "        \n",
    "        if SHOWIMAGE:\n",
    "            # Display the image with the predicted class name\n",
    "            image = np.array(image)\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "            plt.text(10, 10, f'Predicted: {predicted_class_name}', fontsize=12, color='white', backgroundcolor='red')\n",
    "            plt.show()\n",
    "        \n",
    "    \n",
    "print(f'{counter = }')\n",
    "print(f'{correct = }')\n",
    "print(f'{wrong = }')\n",
    "print(f'{correct_leaf = }')\n",
    "print(f'{wrong_leaf = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "108a5823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy\n",
    "# from sklearn import metrics\n",
    "\n",
    "# actual = numpy.random.binomial(1,.9,size = 1000)\n",
    "# predicted = numpy.random.binomial(1,.9,size = 1000)\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "\n",
    "# cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "# cm_display.plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1303f",
   "metadata": {},
   "source": [
    "pth - Class   | Leaf\n",
    "10E - 400, 70 | 442, 28 \n",
    "50E - 423, 47 | 453, 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7e18e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
